{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Stimulus Check Tweet Pull Using Python and Tweepy\n\nThe below code connects to the twitter API and will get a stream of tweets that incluse the Lidl Keyword."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: tweepy in /opt/conda/envs/Python36/lib/python3.6/site-packages (3.8.0)\nRequirement already satisfied: requests>=2.11.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tweepy) (2.21.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tweepy) (1.3.0)\nRequirement already satisfied: PySocks>=1.5.7 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tweepy) (1.6.8)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tweepy) (1.12.0)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests>=2.11.1->tweepy) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests>=2.11.1->tweepy) (2020.4.5.1)\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests>=2.11.1->tweepy) (1.24.1)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests>=2.11.1->tweepy) (2.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\nRequirement already satisfied: textblob in /opt/conda/envs/Python36/lib/python3.6/site-packages (0.15.3)\nRequirement already satisfied: nltk>=3.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from textblob) (3.4)\nRequirement already satisfied: six in /opt/conda/envs/Python36/lib/python3.6/site-packages (from nltk>=3.1->textblob) (1.12.0)\nRequirement already satisfied: singledispatch in /opt/conda/envs/Python36/lib/python3.6/site-packages (from nltk>=3.1->textblob) (3.4.0.3)\n[nltk_data] Downloading package brown to /home/dsxuser/nltk_data...\n[nltk_data]   Package brown is already up-to-date!\n[nltk_data] Downloading package punkt to /home/dsxuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/dsxuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package conll2000 to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Package conll2000 is already up-to-date!\n[nltk_data] Downloading package movie_reviews to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Package movie_reviews is already up-to-date!\nFinished.\n"
                },
                {
                    "data": {
                        "text/plain": "{'file_name': 'Stimulus_Tweets.csv',\n 'message': 'File saved to project storage.',\n 'bucket_name': 'twittersentimentanalysis-donotdelete-pr-ydy3u38p5q0hxs',\n 'asset_id': 'eddf0853-0f1d-46d2-8d8c-66d4b922d811'}"
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# Import the Tweepy library for using the twitter API and the library for sentiment analysis\n!pip install tweepy\n!pip install textblob\n!python -m textblob.download_corpora\nimport tweepy\nfrom textblob import TextBlob\nimport pandas as pd\nimport re \n\n# Create all the necessary authentication variables\nconsumer_key = \#Your Key\nconsumer_secret = \#Your Secret\"\naccess_token = \#Your Token\"\naccess_token_secret = \#Your Token Secret\"\n\n# Creating the authentication object\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n# Setting the access token and secret\nauth.set_access_token(access_token, access_token_secret)\n# Creating the API object while passing in auth information\napi = tweepy.API(auth) \n\n# The search term you want to find\nquery = \"stimulus check OR trump check OR stimulus package\"\n# Language code (follows ISO 639-1 standards)\nlanguage = \"en\"\n# Number of Tweets to pull\ncnt = 100\n# Result type (default is mixed)\nres = \"mixed\"\n\n# Calling the user_timeline function with our parameters\nresults = api.search(q=query, lang=language,count=cnt,result_type=res)\n\n# Function to clean unwanted characters out of a tweets text\ndef clean_tweet(tweet): \n\n    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n\n# Function to get a sentiment score for a tweet\ndef get_tweet_sentiment(tweet): \n\n    # create TextBlob object of passed tweet text \n    text = clean_tweet(tweet)\n    analysis = TextBlob(text) \n    # set sentiment\n    if analysis.sentiment.polarity > 0: \n        return ['positive',analysis.sentiment.polarity]\n    elif analysis.sentiment.polarity == 0: \n        return ['neutral',analysis.sentiment.polarity]\n    else: \n        return ['negative',analysis.sentiment.polarity]\n\n# Create the data frame\ndf_tweets = pd.DataFrame(columns=[\"Tweet ID\", \"Username\", \"Tweet Text\", \"User Profile Location\", \"Number of Retweets\", \"Number of Favorites\", \"Sentiment Score\", \"Sentiment Category\", \"Tweet Date\"])\ni = 0\n\n# Loop over all tweets and get the important data + the sentiment of each tweet\nfor tweet in results:\n    # Get all of the data we want\n    Tid = tweet.id\n    User = tweet.user.screen_name\n    Body = tweet.text\n    Location = tweet.user.location\n    RTctn = tweet.retweet_count\n    Favctn = tweet.favorite_count\n    CreDate = tweet.created_at\n    \n    if Body.startswith(\"RT\"):\n        continue\n    \n    Senti = get_tweet_sentiment(Body)\n    SScore = Senti[1]\n    Senti = Senti[0]\n    TweetRow = [Tid, User, Body, Location, RTctn, Favctn, SScore, Senti, CreDate]\n    df_tweets.loc[i] = TweetRow\n    i=i+1\n    \n    \n# Read in previous csv file from the project\nmy_file = project.get_file(\"Stimulus_Tweets.csv\")\nmy_file.seek(0)\ndf_full_tweets = pd.read_csv(my_file)\n\n# Append the new tweets to the old file\ndf_full_tweets = df_full_tweets.append(df_tweets)\n\n# Remove Duplicates\ndf_full_tweets = df_full_tweets.drop_duplicates(subset =\"Tweet ID\")\n\n# Save the updated tweets file with the new tweets\nproject.save_data(\"Stimulus_Tweets.csv\", df_full_tweets.to_csv(index=False), overwrite=True) "
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
